# Локальный AI-агент с n8n, Ollama и RAG

Этот проект представляет собой полностью автономного AI-агента, которого вы можете запустить на собственном оборудовании. Он использует n8n для оркестрации, Ollama для работы с локальной LLM, ChromaDB для долговременной памяти (RAG) и Tavily для поиска в интернете. Агент доступен через Telegram-бота.

## Возможности

- **Локализация:** Ваши данные остаются на вашей машине. Нет зависимости от облачных LLM.
- **Долговременная память:** Агент помнит прошлые разговоры, используя RAG-пайплайн с ChromaDB.
- **Поиск в интернете:** Агент может искать информацию в интернете с помощью Tavily для ответов на актуальные вопросы.
- **Простое развертывание:** Весь стек описан в одном файле `docker-compose.yml`.
- **Безопасность:** Вебхук n8n доступен по HTTPS с использованием Traefik и Let's Encrypt.
- **Оптимизация для слабых систем:** Проект разработан для запуска на ноутбуке с ограниченными ресурсами CPU и RAM.

## Архитектура

```
+----------------+      +----------------+      +-----------------+      +-----------------+
|                |      |                |      |                 |      |                 |
|  Telegram-бот  |----->|  n8n Workflow  |----->|  Ollama (LLM)   |      |  Поиск Tavily   |
|                |      |                |      |                 |      |                 |
+----------------+      +-------+--------+      +-----------------+      +--------+--------+
                              |                                                |
                              |                                                |
                              v                                                |
                      +-------+--------+                                       |
                      |                |                                       |
                      |  Chroma (RAG)  |<--------------------------------------+
                      |                |
                      +----------------+
```

## Требования

- Установленные Docker и Docker Compose
- Доменное имя
- Публичный (белый) IP-адрес

## Установка

1.  **Клонируйте репозиторий:**
    ```bash
    git clone <URL-репозитория>
    cd <имя-репозитория>
    ```

2.  **Настройте ваше окружение:**
    - Скопируйте файл `.env.example` в `.env`:
      ```bash
      cp .env.example .env
      ```
    - Отредактируйте файл `.env` и заполните следующие значения:
      - `N8N_HOST`: Ваше доменное имя для n8n (например, `n8n.your-domain.com`).
      - `N8N_ENCRYPTION_KEY`: Длинная, случайная строка для шифрования учетных данных в n8n.
      - `WEBHOOK_URL`: Полный URL вашего инстанса n8n (например, `https://n8n.your-domain.com`).
      - `GENERIC_TIMEZONE`: Ваш часовой пояс (например, `Europe/Moscow`).
      - `TELEGRAM_TOKEN`: Токен вашего Telegram-бота.
      - `TAVILY_API_KEY`: Ваш API-ключ от Tavily.
      - `OLLAMA_MODEL`: Имя модели Ollama (например, `llama3.2:3b`).
      - `OLLAMA_BASE_URL`: Базовый URL API Ollama. Для локального инстанса — `http://ollama:11434`. Для облака — например, `https://api.ollama.ai`.
      - `OLLAMA_API_KEY`: Токен доступа к облачному API Ollama (если используется Ollama Cloud). Передается в заголовке `Authorization: Bearer <ключ>`.
      - `ACME_EMAIL`: Ваш email для получения уведомлений от Let's Encrypt.

3.  **Настройте DNS:**
    - В настройках вашего домена создайте **A-запись**, которая будет указывать с вашего `N8N_HOST` на публичный IP-адрес вашего сервера.

4.  **Запустите сервисы:**
    ```bash
    docker-compose up -d
    ```

5.  **Настройте n8n:**
    - Откройте n8n в вашем браузере (например, `https://n8n.your-domain.com`).
    - Создайте новый workflow и импортируйте файл `n8n/workflows/ai-agent.json`.
    - В импортированном workflow вам нужно настроить учетные данные (credentials) для Telegram, Chroma, Ollama и Tavily.
      - **Telegram:** Создайте новые "Telegram" credentials и введите токен вашего бота.
      - **Chroma:** Создайте новые "Chroma" credentials. URL должен быть `http://chroma:8000`.
      - **Ollama:** Создайте новые "Ollama" credentials. URL должен быть `http://ollama:11434` для локального варианта или используйте `OLLAMA_BASE_URL` для подключения к облаку. При работе с Ollama Cloud добавьте `OLLAMA_API_KEY`.
      - **Tavily:** Создайте новые "Tavily" credentials и введите ваш API-ключ от Tavily.
    - Активируйте workflow (переключатель "Active" вверху справа).

6.  **Установите вебхук для Telegram:**
    - В workflow n8n скопируйте URL из узла "Webhook".
    - Отправьте POST-запрос к Telegram API, чтобы установить вебхук для вашего бота. Это можно сделать с помощью `curl`:
      ```bash
      curl -F "url=<URL-вашего-n8n-вебхука>" https://api.telegram.org/bot<токен-вашего-бота>/setWebhook
      ```

7.  **Начинайте общаться с вашим AI-агентом!**

## Использование

### Основной режим (RAG)
Просто пишите сообщения боту. Он будет искать ответы в своей локальной памяти (ChromaDB) и отвечать, используя этот контекст.

### Поиск в интернете
Чтобы найти актуальную информацию в интернете, используйте команду `/search`:
```text
/search <ваш запрос>
```
Например:
- `/search курс биткоина сегодня`
- `/search кто победил на евровидении 2024`

Агент выполнит поиск через Tavily, проанализирует результаты и сформирует ответ.

### Использование Ollama Cloud
- Установите `OLLAMA_BASE_URL` на адрес облачного API (например, `https://api.ollama.ai`) и задайте `OLLAMA_API_KEY`.
- Установите модель в `OLLAMA_MODEL` (например, `llama3.2:3b`).
- В workflow HTTP-запросы к Ollama автоматически используют указанный базовый URL и добавляют заголовок авторизации при наличии ключа.

## Отладка

- **Просмотр логов контейнера:**
  ```bash
  docker-compose logs -f <имя-сервиса>
  ```
  (например, `docker-compose logs -f n8n`)

- **Мониторинг использования ресурсов:**
  ```bash
  docker stats
  ```

- **Панель управления Traefik:**
  - Откройте `http://<IP-вашего-сервера>:8080` в браузере, чтобы увидеть панель управления Traefik.

## Типичные ошибки

- **"502 Bad Gateway":** Обычно это означает, что контейнер n8n не запущен или недоступен для Traefik. Проверьте логи обоих контейнеров.
- **"404 Not Found":** Может означать, что URL вебхука указан неверно или workflow не активирован.
- **Ошибки Ollama:** Если у вас проблемы с Ollama, вы можете попробовать запустить его в режиме отладки, добавив переменную окружения `OLLAMA_DEBUG=1` в `docker-compose.yml`.

## Как добавлять новые узлы

Workflow в n8n спроектирован как модульный. Вы можете легко добавлять новую функциональность, добавляя новые узлы в цепочку. Например, вы можете добавить узел для отправки email или взаимодействия с другим API.
